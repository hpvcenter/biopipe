{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kraken2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "import pysam\n",
    "import os\n",
    "from hops import hdfs\n",
    "import utils\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "import subprocess\n",
    "import stat\n",
    "import gzip\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#args_full = utils.load_arguments([0,\"-shdfs:///Projects/HPV_meta/Jupyter/Bio_pipeline/settings/settings_kidney.yml\", \"-ihdfs:///Projects/HPV_meta/Study2/Kidney_study/input/10_sorted.csv\", \"-ohdfs:///Projects/HPV_meta/Study2/Kidney_study/run_dir/benchmark10\"])\n",
    "\n",
    "args_full=utils.load_arguments(sys.argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Temporary input folder, input files if list are looked up in this dir\n",
    "NONHUMAN_DIR = '/Projects/HPV_meta/Backup/TCGA/tcga/nonhuman_all/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "OUTPUT_DATASET=args_full[utils.OUTPUT_DATASET]\n",
    "INPUT_ROOT_PATH=args_full[utils.INPUT_ROOT_PATH]\n",
    "RUN_FOLDER=args_full[utils.RUN_FOLDER]\n",
    "WORK_PATH=os.path.join(OUTPUT_DATASET, RUN_FOLDER)\n",
    "args=args_full['Kraken']\n",
    "\n",
    "\n",
    "# check of input and output root override\n",
    "if args_full.get(utils.INPUT_OVERRIDE):\n",
    "    inputRoot=args_full.get(utils.INPUT_OVERRIDE)\n",
    "else :\n",
    "    inputRoot=os.path.join(WORK_PATH,args['INPUT_ROOT'])\n",
    "if args_full.get(utils.OUTPUT_OVERRIDE):\n",
    "    outputRoot=args_full.get(utils.OUTPUT_OVERRIDE)\n",
    "else:\n",
    "    outputRoot=os.path.join(WORK_PATH,args['OUTPUT_ROOT'])\n",
    "\n",
    "\n",
    "kraken_path=args['KRAKEN_PATH']\n",
    "tool=os.path.basename(kraken_path)\n",
    "kk_db_path=args['KRAKEN_DB_PATH']\n",
    "is_save_all_outputs=args['SAVE_FULL_OUTPUT']\n",
    "threads=args['THREADS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install kraken from hdfs source\n",
    "def load_kraken(kraken_path):\n",
    "    tool=os.path.basename(kraken_path)\n",
    "    \n",
    "    hdfs.copy_to_local(kraken_path)\n",
    "\n",
    "    st = os.stat(tool+'/kraken2')\n",
    "    os.chmod(tool+'/kraken2', st.st_mode | stat.S_IEXEC)\n",
    "    \n",
    "    st = os.stat(tool+'/classify')\n",
    "    os.chmod(tool+'/classify', st.st_mode | stat.S_IEXEC)\n",
    "\n",
    "\n",
    "\n",
    "def compress_file(file):\n",
    "    compress_file=file+'.gz'\n",
    "    with open(file, 'rb') as f_in:        \n",
    "        with gzip.open(compress_file, 'wb',compresslevel=1) as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "            \n",
    "    return compress_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def apply_kraken_single(file_path,kk_db_path):\n",
    "    \"\"\"\n",
    "    Runs kraken on single file via subprcess.\n",
    "    First kraken is installed by copying kraken tool from hdfs.\n",
    "    Outputs are copied back to hdfs.\n",
    "    If an output file name is already present in output directory the processing\n",
    "    of file is skipped to avoid processing of same file in case of resubmit of failed run.\n",
    "\n",
    "    :param file_path:\n",
    "    :param kk_db_path:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    file=os.path.split(file_path)[1]  \n",
    "    sample=os.path.splitext(os.path.splitext(file)[0])[0]    \n",
    "    report=sample+'_report.txt'\n",
    "    \n",
    "    if not hdfs.exists(os.path.join(outputRoot,'report',report)): # check if output already exists\n",
    "        \n",
    "        if not hdfs.exists(file_path):      \n",
    "            print(\"Input file not found, skipping to next\")\n",
    "            return None\n",
    "        \n",
    "        hdfs.copy_to_local(file_path)\n",
    "        # install kraken\n",
    "        load_kraken(kraken_path) \n",
    "        kk_db=os.path.split(kk_db_path)[1]  \n",
    "        hdfs.copy_to_local(kk_db_path, overwrite=True)\n",
    "\n",
    "        output=sample+'_out.txt'\n",
    "        unclassified=sample+'_unclassified.txt'\n",
    "        if is_save_all_outputs: # save unclassified and output files\n",
    "            params={'--db':kk_db,'--threads': threads, '--report': report,'--report-minimizer-data':'','--report-zero-counts':'','--unclassified-out': unclassified, file: '','--output': output, \"--memory-mapping\":\"\" }\n",
    "        else :\n",
    "            params={'--db':kk_db,'--threads': threads, '--report': report,'--report-minimizer-data':'','--report-zero-counts':'','--unclassified-out': '/dev/null', file: '','--output': '/dev/null',\"--memory-mapping\":\"\" }\n",
    "        cmd=utils.build_command(tool+'/kraken2',params)\n",
    "        print(cmd)\n",
    "        try:\n",
    "            status=subprocess.run(cmd.split(),stdout=subprocess.PIPE,check=True)\n",
    "\n",
    "            if status.returncode==0 and os.path.exists(report):\n",
    "                hdfs.copy_to_hdfs(report,os.path.join(outputRoot,'report'),overwrite=True)\n",
    "                os.remove(report)\n",
    "\n",
    "                if is_save_all_outputs:\n",
    "                    # compress\n",
    "                    c_output=compress_file(output)\n",
    "                    c_unclassified=compress_file(unclassified)\n",
    "                    # copy to hdfs\n",
    "                    hdfs.copy_to_hdfs(c_unclassified,os.path.join(outputRoot,'unclassified'), overwrite=True)\n",
    "                    hdfs.copy_to_hdfs(c_output,os.path.join(outputRoot,'output'), overwrite=True)\n",
    "                    # remove local files\n",
    "                    os.remove(output)\n",
    "                    os.remove(unclassified)\n",
    "                    os.remove(c_output)\n",
    "                    os.remove(c_unclassified)\n",
    "\n",
    "\n",
    "            return file\n",
    "        except subprocess.CalledProcessError:\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "        finally:\n",
    "             os.remove(file)\n",
    "    else :\n",
    "        print('skipping existing file: ', file)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all input file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_files=0\n",
    "\n",
    "inputFiles=[]\n",
    "inputFiles=[]\n",
    "if hdfs.isfile(inputRoot):\n",
    "    print(\" --- Input: is file ----reading from input file list\")\n",
    "    df= pd.read_csv(inputRoot)\n",
    "    number_of_files=df.count()\n",
    "    inputFiles = df[df.columns[0]].tolist()\n",
    "    \n",
    "else:\n",
    "    print(\" --- Input: is folder ----reading from input folder\")\n",
    "    inputFiles=utils.load_file_names(inputRoot)\n",
    "    number_of_files=len(inputFiles)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of input files {}\".format(len(inputFiles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def renameFiles(x):\n",
    "    x = NONHUMAN_DIR+x\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_names=sc.parallelize(inputFiles,sc.getConf().get(\"spark.executor.instances\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# run\n",
    "# final=rdd_names.map(lambda x: renameFiles(x)).map(lambda x: apply_kraken_single(x,kk_db_path) ).collect()\n",
    "final=rdd_names.map(lambda x: apply_kraken_single(x,kk_db_path) ).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mega",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "python",
   "pygments_lexer": "python3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
